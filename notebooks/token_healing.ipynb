{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token healing\n",
    "\n",
    "Guidance uses what we call \"token healing\" to fix tokenization artifacts that normally arise at the boundary between the end of a prompt and the beginning of a set of generated tokens. Note that token healing requires direct endpoint integration to run effciently, so it currently supported only for the `guidance.llms.Transformers` LLM backend.\n",
    "\n",
    "## Why token healing is needed\n",
    "Language models process tokens, which are chunks of text that often are similar to a word. This impacts how language models see text, and also how we can prompt them, since every prompt has to be a set of tokens. Encodings like [BPE](https://en.wikipedia.org/wiki/Byte_pair_encoding) that are used by GPT-style models map all input bytes to token ids in a greedy manner. This works well during training, but can lead to some subtle issues during prompting and inference because the greedy token boundaries often don't line up with the end of the prompt if we consider the generated tokens that will also come next. Of course the end of a prompt will always align with a token boundary because the prompt is tokenized before being extended by the model, but if the first characters of the completion are part of a longer token that would span the prompt boundary, that longer token cannot be used (even though that is what the model would expect based on training data). \n",
    "\n",
    "To see why token healing is important consider the prompt \"This is a \", which is then completed with \"fine day.\" by the model, so resulting in the final string \"This is a fine day.\". If we tokenize the prompt \"This is a \" with GPT2 BPE we get `[1212, 318, 257, 220]`, and the tokenization of the extention \"fine day.\" is `[38125, 1110, 13]`. This results in a final prompt + generation token sequence of `[1212, 318, 257, 220, 38125, 1110, 13]`. If however we were to tokenize the whole string \"This is a fine day.\" jointly we instead get `[1212, 318, 257, 3734, 1110, 13]`. Which tokenization is correct? Well, the correct tokenization is the one that best communicates intent to the model. Since the model learned intent based on a greedy tokenization of the training text, that means the joint tokenization that also uses pure greedy matching will better align with how the model processed the training data, and so it is also likely to better communicate intent to the model. This is the reason why ending your prompt with a space is almost always a bad idea in GPT models since most word-based tokens have the space before the word, not after it.\n",
    "\n",
    "Note that another way to see that the \"standard\" prompt-boundary-based encoding is worse than the joint one we get with token healing is to observe that 38125 (the token id for \"fine\") is a large number, this means it is uncommon to see that token in the training data (since BPE encodings are built up greedily based on frequency). In contrast 3734 (the token id for \" fine\") is a much more common token and so more likely to clearly communicate intent to the model (since the model has seen it many times and hence had more opportunity to learn its meaning in many contexts).\n",
    "\n",
    "## How token healing works\n",
    "Guidance avoids the above tokenization artifacts automatically using a method we call \"token healing\" that backs up the generation process by one token before the end of the prompt, then constrains the first token generated to have a prefix that matches the last token in the prompt. This allows the generated text string to have the token encoding that the model would expect based on its training data, not an unusual alternative encoding forcing by the prompt boundary. Token healing allows you to express your prompts however you wish, without worrying about boundaries (which effect many tokens, not just space characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization of `This is a `: [1212, 318, 257, 220]\n",
      "Tokenization of `fine day.`: [38125, 1110, 13]\n",
      "Tokenization of `This is a fine day.`: [1212, 318, 257, 3734, 1110, 13]\n"
     ]
    }
   ],
   "source": [
    "# compute the tokenizations of the example above\n",
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2')\n",
    "print(\"Tokenization of `This is a `:\", tokenizer.encode(\"This is a \"))\n",
    "print(\"Tokenization of `fine day.`:\", tokenizer.encode(\"fine day.\"))\n",
    "print(\"Tokenization of `This is a fine day.`:\", tokenizer.encode(\"This is a fine day.\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token healing in action\n",
    "\n",
    "Below is a prompt that we run both with and without token healing to see how it can impact generation quality.\n",
    "\n",
    "### With token healing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>Tweak this proverb to apply to model instructions instead.\n",
       "\n",
       "Where there is no guidance, a people falls,\n",
       "but in an abundance of counselors there is safety.\n",
       "- Proverbs 11:14\n",
       "\n",
       "UPDATED\n",
       "Where there is no<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> guidance</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>,</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> a</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> people</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> falls</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>,</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>but</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> in</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> an</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> abundance</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> of</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> counselors</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> there</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> is</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> safety</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>.</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>\n",
       "</span>- GPT Proverbs <span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>11</span>:<span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>14</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import guidance\n",
    "llm = guidance.models.Transformers(\"gpt2\", temperature=0.8)\n",
    "# guidance.llms.Transformers.cache.clear()\n",
    "\n",
    "@guidance\n",
    "def proverb_tweaker(lm, *, proverb, book, chapter, verse):\n",
    "    lm += \"Tweak this proverb to apply to model instructions instead.\\n\\n\"\n",
    "    lm += proverb + \"\\n\"\n",
    "    lm += f\"- {book} {chapter}:{verse}\"\n",
    "    lm += \"\\n\\n\"\n",
    "    lm += \"UPDATED\\n\"\n",
    "    lm += \"Where there is no guidance\" + guidance.gen(name='rewrite', stop='- ')\n",
    "    lm += \"- GPT Proverbs \"\n",
    "    lm += guidance.gen(name='chapter', max_tokens=10, stop=\":\")\n",
    "    lm += \":\"\n",
    "    lm += guidance.gen(name='verse', stop=\"\\n\")\n",
    "\n",
    "    return lm\n",
    "\n",
    "'''\n",
    "program = guidance(\"\"\"Tweak this proverb to apply to model instructions instead.\n",
    "\n",
    "{{proverb}}\n",
    "- {{book}} {{chapter}}:{{verse}}\n",
    "\n",
    "UPDATED\n",
    "Where there is no guidance{{gen 'rewrite' stop=\"- \"}}\n",
    "- GPT Proverbs {{gen 'chapter' max_tokens=10}}:{{gen 'verse' stop='\\\\n'}}\"\"\", llm=llm)\n",
    "'''\n",
    "\n",
    "# execute the program on a specific proverb\n",
    "tweaked = llm + proverb_tweaker(\n",
    "    proverb=\"Where there is no guidance, a people falls,\\nbut in an abundance of counselors there is safety.\",\n",
    "    book=\"Proverbs\",\n",
    "    chapter=11,\n",
    "    verse=14\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without token healing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import guidance\n",
    "llm = guidance.models.Transformers(\"gpt2\", token_healing=False)\n",
    "# guidance.llms.Transformers.cache.clear()\n",
    "program = guidance(\"\"\"Tweak this proverb to apply to model instructions instead.\n",
    "\n",
    "{{proverb}}\n",
    "- {{book}} {{chapter}}:{{verse}}\n",
    "\n",
    "UPDATED\n",
    "Where there is no guidance{{gen 'rewrite' stop=\"- \"}}\n",
    "- GPT Proverbs {{gen 'chapter' max_tokens=10}}:{{gen 'verse' stop='\\\\n'}}\"\"\", llm=llm)\n",
    "\n",
    "# execute the program on a specific proverb\n",
    "program(\n",
    "    proverb=\"Where there is no guidance, a people falls,\\nbut in an abundance of counselors there is safety.\",\n",
    "    book=\"Proverbs\",\n",
    "    chapter=11,\n",
    "    verse=14\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it may seems strange to us why GPT2 does not put a number after the space character (instead writing a bunch of underscores), it makes sense if we think about the tokens involved. If a number was likely to come after the space, then it would have been included in the token. By sending the token id 220 (a space) by itself to the model we are communicating that what comes next is not something that is in a token that starts with a space (since otherwise the greedy tokenization would have used it). So GPT2 picks a non-word token (which are not normally prefixed with a space). In this case it chooses 2602, which is a bunch of underscores \"________\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height: 1px; opacity: 0.5; border: none; background: #cccccc;\">\n",
    "<div style=\"text-align: center; opacity: 0.5\">Have an idea for more helpful examples? Pull requests that add to this documentation notebook are encouraged!</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
